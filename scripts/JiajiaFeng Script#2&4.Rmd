

```{r}
load('data2/claims-raw.RData')
```

```{r}
ls()
head(claims_raw)
```

```{r}
library(rvest)
library(tidytext)
library(dplyr)

data("stop_words")
```

## Preliminary Task1

### Scrape both header and paragraph content
```{r}
library(rvest)

# use tryCatch: prevent the entire script from failing due to an issue with a single piece of HTML content
extract_content <- function(html_content) {
  tryCatch({
    # Read the HTML content
    page <- read_html(html_content)
    
    # Extract headers and paragraphs
    headers <- page %>% html_nodes("h1, h2, h3") %>% html_text()
    paragraphs <- page %>% html_nodes("p") %>% html_text()
    
    # Combine headers and paragraphs
    combined_content <- c(headers, paragraphs)
    return(paste(combined_content, collapse = " "))
  }, error = function(e) {
    # Return NA or an empty string if an error occurs
    return(NA)
  })
}

# Apply the function to the 'text_tmp' column
claims_raw$content_combined <- sapply(claims_raw$text_tmp, extract_content)

```

### Tokenize the text
```{r}
# Tokenize the text data
tokenized_data <- claims_raw %>%
  unnest_tokens(word, content_combined) %>%
  anti_join(stop_words, by = "word")  # Remove stop words

# Create a document-term matrix (DTM)
dtm <- tokenized_data %>%
  count(.id, word) %>%
  cast_dtm(document = .id, term = word, value = n)

```

