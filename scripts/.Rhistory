) %>% head()
atlantic %>%
separate_wider_regex(
cols = latitude,
patterns = c(latitude = "[0-9.]+", direction = "[NS]")
) %>%
mutate(
latitude = as.numeric(latitude)
) %>%
mutate(
latitude = case_when(
direction == 'S' ~ -1*latitude,
.default = latitude
)
) %>%
select(
-direction
) %>%
separate_wider_regex(
cols = longitude,
patterns = c(longitude = "[0-9.]+", direction = "[EW]")
) %>%
mutate(
longitude = as.numeric(longitude)
) %>%
mutate(
longitude = case_when(
direction == 'W' ~ -1*longitude,
.default = longitude
)
) %>%
select(
-direction
) %>%
mutate(
year = as.integer(year),
month = factor(month, ordered = TRUE),
day = factor(day, ordered = TRUE),
status = factor(status),
event = factor(event)
) %>% head()
atlantic <- atlantic %>%
separate_wider_regex(
cols = latitude,
patterns = c(latitude = "[0-9.]+", direction = "[NS]")
) %>%
mutate(
latitude = as.numeric(latitude)
) %>%
mutate(
latitude = case_when(
direction == 'S' ~ -1*latitude,
.default = latitude
)
) %>%
select(
-direction
) %>%
separate_wider_regex(
cols = longitude,
patterns = c(longitude = "[0-9.]+", direction = "[EW]")
) %>%
mutate(
longitude = as.numeric(longitude)
) %>%
mutate(
longitude = case_when(
direction == 'W' ~ -1*longitude,
.default = longitude
)
) %>%
select(
-direction
) %>%
mutate(
year = as.integer(year),
month = factor(month, ordered = TRUE),
day = factor(day, ordered = TRUE),
status = factor(status),
event = factor(event)
)
atl_wind_pres %>%
ggplot(aes(
x = mean_max_wind,
y = mean_min_pres
)) %+%
geom_point(
na.rm = TRUE
)
atlantic %>%
select(
id
) %>%
unique()
install.packages("leaflet")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
courseGrades <- read_csv("courseGrades.csv")
years <- str_sub(as.character(2009:2024), -2)
qs <- c("W", "S", "M", "F")
qy <- paste(qs, rep(years, each = length(qs)), sep = "")
courseGrades <- courseGrades %>%
mutate(
isCCS = factor(ifelse(str_detect(course, "\\sCS\\s|CMPSCCS|TGCS"), "Yes", "No"))
) %>%
mutate(
preCOVID = factor(
ifelse((year < 2020) | ((year == 2020) & quarter == "Winter"), 1, 0),
levels = c(0, 1),
labels = c("No", "Yes")
)
) %>%
mutate(
courseLevel = factor(
case_when(
str_detect(course, "\\s+[0-9]{1,2}[a-zA-Z]*$") ~ "UGLD",
str_detect(course, "\\s+1[0-9]{2}[a-zA-Z]*$") ~ "UGUD",
str_detect(course, "\\s+[A-Z]*[2-9][0-9]{2}[a-zA-Z]*$") ~ "GRAD"
),
levels = c("UGLD", "UGUD", "GRAD")
)
) %>%
mutate(
department = str_extract(course, "^[A-Z &]+\\b") %>% str_squish()
) %>%
mutate(
online = if_else(
str_detect(department, ".W$"),
"Yes",
"No"
) %>%
factor()
) %>%
mutate(
department = if_else(
str_detect(department, ".W$"),
str_remove(department, "(?<=.)W$") %>% str_squish(),
department
)
) %>%
mutate(
department = if_else(
str_detect(department, "^ED "),
"ED",
department
)
) %>%
mutate(
quarter = factor(
quarter,
levels = c("Winter", "Spring", "Summer", "Fall"),
ordered = TRUE
)
) %>%
mutate(
quarterYear = paste(
case_when(
quarter == "Winter" ~ "W",
quarter == "Spring" ~ "S",
quarter == "Summer" ~ "M",
quarter == "Fall" ~ "F"
),
str_sub(year, -2),
sep = ""
) %>%
factor(
levels = qy,
ordered = TRUE
)
)
courseGrades %>%
head()
courseGrades <- read_csv("courseGrades.csv")
courseGrades %>%
head()
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
library(sparsesvd)
library(glmnet)
source("../scripts/modified-preprocessing.R")
setwd("~/Documents/PSTAT-197/module-2-group1/scripts")
source("../scripts/modified-preprocessing.R")
source("./scripts/projection.R")
source("../scripts/projection.R")
# load raw data
load('../data/claims-raw.RData')
# clean raw data
claims_clean <- claims_raw %>%
parse_data()
nlp_fn <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = bigram,
input = text_clean,
token = 'ngrams',
n = 2,
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(bigram.lem = lemmatize_strings(bigram)) %>%
filter(str_length(bigram.lem) > 2) %>%
count(.id, bclass, mclass, bigram.lem, name = 'n') %>%
bind_tf_idf(term = bigram.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
names_from = 'bigram.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
nlp_fn <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = bigram,
input = text_clean,
token = 'ngrams',
n = 2,
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(bigram.lem = lemmatize_strings(bigram)) %>%
filter(str_length(bigram.lem) > 2) %>%
count(.id, bclass, mclass, bigram.lem, name = 'n') %>%
bind_tf_idf(term = bigram.lem,
document = .id,
n = n) # %>%
# pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
#            names_from = 'bigram.lem',
#            values_from = 'tf_idf',
#            values_fill = 0)
return(out)
}
# get bigrams dtm with labels
claims_bigrams <- claims_clean %>%
nlp_fn()
claims_bigrams <- claims_bigrams %>%
pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
names_from = 'bigram.lem',
values_from = 'tf_idf',
values_fill = 0)
mem.maxVSize()
? mem.maxVSize()
mem.maxVSize(Inf)
mem.maxVSize()
claims_bigrams <- claims_bigrams %>%
pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
names_from = 'bigram.lem',
values_from = 'tf_idf',
values_fill = 0)
claims_bigrams %>% names() %>% length()
claims_bigrams %>% nrow()
claims_clean %>% nrow()
nlp_fn <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = bigram,
input = text_clean,
token = 'ngrams',
n = 2,
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(bigram.lem = lemmatize_strings(bigram)) %>%
filter(str_length(bigram.lem) > 2) %>%
count(.id, bclass, mclass, bigram.lem, name = 'n') %>%
bind_tf_idf(term = bigram.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
names_from = 'bigram.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
save(claims_bigrams, file = '../data/claims-bigrams.RData')
save(log_odds, file = "../data/log-odds.R")
# load raw data
load('../data/claims-raw.RData')
# clean raw data
claims_clean <- claims_raw %>%
parse_data()
# apply nlp to get DTM with labels
claims <- claims_clean %>%
nlp_fn()
source("../scripts/modified-preprocessing.R")
# apply nlp to get DTM with labels
claims <- claims_clean %>%
nlp_fn()
set.seed(11182024)
# partition claims data
partitions <- claims %>% initial_split(prop = 0.8)
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
library(sparsesvd)
library(glmnet)
# partition claims data
partitions <- claims %>% initial_split(prop = 0.8)
? initial_split()
?? initial_split
library(tidymodels)
# partition claims data
partitions <- claims %>% initial_split(prop = 0.8)
# separate DTM from labels
test_dtm <- testing(partitions) %>%
select(-.id, -bclass, -mclass) %>%
as.matrix()
test_labels <- testing(partitions) %>%
select(.id, bclass, mclass)
# same, training set
train_dtm <- training(partitions) %>%
select(-.id, -bclass, -mclass) %>%
as.matrix()
train_labels <- training(partitions) %>%
select(.id, bclass, mclass)
# PCA/projection
proj_out <- projection_fn(train_dtm, 0.7)
train_dtm_projected <- proj_out$data
# Fit Regression
train <- train_labels %>%
transmute(bclass = factor(bclass)) %>%
bind_cols(train_dtm_projected)
# store predictors and response as matrix and vector
x_train <- train %>% select(-bclass) %>% as.matrix()
y_train <- train_labels %>% pull(bclass)
# fit enet model
alpha_enet <- 0.3
fit_reg <- glmnet(x = x_train,
y = y_train,
family = 'binomial',
alpha = alpha_enet)
# choose a constraint strength by cross-validation
cvout <- cv.glmnet(x = x_train,
y = y_train,
family = 'binomial',
alpha = alpha_enet)
# store optimal strength
lambda_opt <- cvout$lambda.min
# get log-odds
log_odds <- predict(fit_reg,
s = lambda_opt,
newx = x_train,
type = "link")
save(log_odds, file = "../data/log-odds.R")
# project test data onto PCs
test_dtm_projected <- reproject_fn(.dtm = test_dtm, proj_out)
# load log odds from binary logistic principal components regression of word-tokenized data
load("../data/log-odds.R")
claims_bigrams$.id[1:5]
claims_bigrams %>%
select(
.id
)
claims_bigrams %>%
select(
.id
) %>%
View()
test_labels
train_labels
test_labels
test_id <- test_labels %>% select(.id)
train_id <- train_labels %>% select(.id)
save(test_id, "../data/test-id.R")
save(log_odds, file = "../data/log-odds.RData")
save(test_id, "../data/test-id.RData")
save(test_id, file = "../data/test-id.RData")
save(train_id, file = "../data/train-id.RData")
load("../data/train-id.RData")
load("../data/train-id.RData")
load("../data/test-id.RData")
# partition claims bigrams
training_bigrams <- claims %>%
filter(
.id %in% train_id
)
train_id
claims_bigrams$.id %>% View()
claims_bigrams %>%
select(
.id
) %>%
View()
# load log odds from binary logistic principal components regression of word-tokenized data
load("../data/log-odds.RData")
gc()
? filter()
cb_id <- claims_bigrams %>%
select(
.id
)
cb_id[1:5] %in% train_id
cb_id[1] %in% train_id
cb_id[2] %in% train_id
cb_id[1:5,] %in% train_id
cb_id %>% head()
cb_id$.id %in% train_id
cb_id$.id[1:5] %in% train_id
cb_id$.id[1:5]
train_id
which(train.id == "url44")
which(train_id == "url44")
which(train_id == "cb_id$.id[1]")
which(train_id == cb_id$.id[1])
which(train_id == cb_id$.id[2])
which(train_id == cb_id$.id[3])
which(train_id == cb_id$.id[4])
which(train_id == cb_id$.id[5])
cb_id$.id[1:5] %in% train_id
cb_id %>% pull(.id) %in% train_id
cb_id %>% pull(.id) %>% is.vector()
cb_id %>% pull(.id) == train_id
cb_id %>% pull(.id) %in% train_id
"url1" %in% train_id
cb_id %>% pull(.id) %>% head()
"url10" %in% train_id
"url100" %in% train_id
"url1001" %in% train_id
"url1003" %in% train_id
which(train_id == "url1")
train_id %in% cb_id$.id
train_id %in% cb_id %>% pull(.id)
train_id %in% (cb_id %>% pull(.id))
which(cb_id %>% pull.id %in% train_id)
which(cb_id %>% pull(.id) %in% train_id)
"url1" %in% cb_id$.id
"url10" %in% cb_id$.id
"url10" %in% train_id
which(train_id == "url10")
train_id[1234]
train_id[1234,]
"url10" %in% train_id$.id
test_id <- test_labels %>% pull(.id)
save(test_id, file = "../data/test-id.RData")
train_id <- train_labels %>% pull(.id)
save(train_id, file = "../data/train-id.RData")
# load id labels for training and test sets
load("../data/train-id.RData")
load("../data/test-id.RData")
which(cb_id$.id %in% train_id)
# load id labels for training and test sets
load("../data/train-id.RData")
load("../data/test-id.RData")
# partition claims bigrams
training_bigrams <- claims %>%
filter(
.id %in% train_id
)
testing_bigrams <- claims %>%
filter(
.id %in% test_id
)
train_id
test_id
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
library(sparsesvd)
library(glmnet)
library(Matrix)
test_bg_dtm <- testing_bigrams %>%
select(-.id, -bclass, -mclass) %>%
as.matrix()
test_labels <- training_bigrams %>%
select(.id, bclass, mclass)
# same, training set
train_bg_dtm <- training_bigrams %>%
select(-.id, -bclass, -mclass) %>%
as.matrix()
train_labels <- training_bigrams %>%
select(.id, bclass, mclass)
# PCA/projection
proj_out_bg <- projection_fn(train_bg_dtm, 0.7)
train_bg_dtm_projected <- proj_out_bg$data
proj_out_bg$_pc
proj_out_bg$n_pc
proj_out$n_pc
(proj_out_bg$var[1:proj_out$n_pc] %>% sum())/(proj_out_bg$var %>% sum())
(proj_out_bg$var[1:proj_out$n_pc,] %>% sum())/(proj_out_bg$var %>% sum())
proj_out_bg$var[1:proj_out$n_pc,] %>% sum()
proj_out_bg$var %>% sum()
? cumsum()
(proj_out_bg$var$var[1:proj_out$n_pc] %>% sum())/(proj_out_bg$var$var %>% sum())
nlp_bg_fn <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = bigram,
input = text_clean,
token = 'ngrams',
n = 2,
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(bigram.lem = lemmatize_strings(bigram)) %>%
filter(str_length(bigram.lem) > 2) %>%
count(.id, bclass, mclass, bigram.lem, name = 'n') %>%
bind_tf_idf(term = bigram.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass', 'mclass'),
names_from = 'bigram.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
