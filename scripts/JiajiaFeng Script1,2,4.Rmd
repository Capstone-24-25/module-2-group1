

```{r}
load('data2/claims-raw.RData')
```

```{r}
ls()
head(claims_raw)
```

```{r}
library(rvest)
library(tidytext)
library(dplyr)
library(tidyverse)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
```

## Preliminary Task1

### 1.Scrape both header and paragraph content
```{r}

# Why use tryCatch: prevent the entire script from failing due to an issue with a single piece of HTML content
extract_content <- function(html_content) {
  tryCatch({
    # Read the HTML content
    page <- read_html(html_content)
    
    # Extract headers and paragraphs
    headers <- page %>% html_nodes("h1, h2, h3") %>% html_text()
    paragraphs <- page %>% html_nodes("p") %>% html_text()
    
    # Combine headers and paragraphs
    combined_content <- c(headers, paragraphs)
    return(paste(combined_content, collapse = " "))
  }, error = function(e) {
    # Return NA or an empty string if an error occurs
    return(NA)
  })
}

# Apply the function to the 'text_tmp' column
claims_raw$content_combined <- sapply(claims_raw$text_tmp, extract_content)

```

### 2.Tokenize the text
```{r}
# Tokenize the text data
data("stop_words")

tokenized_data <- claims_raw %>%
  unnest_tokens(word, content_combined) %>%
  anti_join(stop_words, by = "word")  # Remove stop words

# Create a document-term matrix (DTM)
dtm <- tokenized_data %>%
  count(.id, word) %>%
  cast_dtm(document = .id, term = word, value = n)

dtm_matrix <- as.matrix(dtm)

```

### 3.Principal Component Analysis (PCA)
```{r}
threshold <- 5 # reduce running time
dtm_filtered <- dtm_matrix[, colSums(dtm_matrix) > threshold]
pca_result <- prcomp(dtm_filtered, center = TRUE, scale. = TRUE)

```

### 4.Logistic Principal Component Regression
```{r}
# fix error: differing number of rows: pca_scores:2120, claims_raw$bclass:2165
rows_to_keep <- rownames(dtm_matrix)
claims_raw_filtered <- claims_raw %>% filter(.id %in% rows_to_keep)
```

```{r}
# Extract PCA scores and combine with the adjusted 'bclass', which is 'binary_class'
pca_scores <- pca_result$x

data_for_model <- data.frame(pca_scores, binary_class = claims_raw_filtered$bclass)
data_for_model$binary_class <- as.factor(data_for_model$binary_class)
```



```{r}
# Fix Warning: glm.fit: algorithm did not converge
binary_class_index <- which(names(data_for_model) == "binary_class")

# Choose the first 500 components (through Cumulative Variance Explained) and binary class column
data_for_model_reduced <- data_for_model[, c(1:500, binary_class_index)] 


names(data_for_model_reduced)
```


```{r}
# Fit logistic regression model using PCA components
logistic_model <- glm(binary_class ~ ., data = data_for_model_reduced, family = binomial)
summary(logistic_model)
```
## 5. Are binary class predictions improved using logistic principal component regression?
```{r}
library(pROC)

predicted_probs <- predict(logistic_model, type = "response")

# AUC after PCA
roc_curve <- roc(data_for_model_reduced$binary_class, predicted_probs)
auc(roc_curve)

# ROC Curve
plot(roc_curve)

```
Yes, the binary class predictions are significantly improved using logistic principal component regression compared to random guessing has an AUC of 0.5. After including 500 principal components, the AUC increased to 0.8268, which demonstrates a strong ability to distinguish between the two classes. 

