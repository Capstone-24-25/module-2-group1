
## Preparation
```{r}
load('data2/claims-raw.RData')
```

```{r}
ls()
```

```{r}
library(rvest)
library(tidytext)
library(dplyr)
library(tidyverse)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
```


## Preliminary Task1 (5 steps)

### 1.Scrape both header and paragraph content
```{r}

# Why use tryCatch: prevent the entire script from failing due to an issue with a single piece of HTML content
extract_content <- function(html_content) {
  tryCatch({
    # Read the HTML content
    page <- read_html(html_content)
    
    # Extract headers and paragraphs
    headers <- page %>% html_nodes("h1, h2, h3") %>% html_text()
    paragraphs <- page %>% html_nodes("p") %>% html_text()
    
    # Combine headers and paragraphs
    combined_content <- c(headers, paragraphs)
    return(paste(combined_content, collapse = " "))
  }, error = function(e) {
    # Return NA or an empty string if an error occurs
    return(NA)
  })
}

# Apply the function to the 'text_tmp' column
claims_raw$content_combined <- sapply(claims_raw$text_tmp, extract_content)

```

### 2.Tokenize the text
```{r}
# Tokenize the text data
data("stop_words")

tokenized_data <- claims_raw %>%
  unnest_tokens(word, content_combined) %>%
  anti_join(stop_words, by = "word")  # Remove stop words

# Create a document-term matrix (DTM)
dtm <- tokenized_data %>%
  count(.id, word) %>%
  cast_dtm(document = .id, term = word, value = n)

dtm_matrix <- as.matrix(dtm)

```

### 3.Principal Component Analysis (PCA)
```{r}
threshold <- 5 # reduce running time
dtm_filtered <- dtm_matrix[, colSums(dtm_matrix) > threshold]
pca_result <- prcomp(dtm_filtered, center = TRUE, scale. = TRUE)

```

### 4.Logistic Principal Component Regression
```{r}
# fix error: differing number of rows: pca_scores:2120, claims_raw$bclass:2165
rows_to_keep <- rownames(dtm_matrix)
claims_raw_filtered <- claims_raw %>% filter(.id %in% rows_to_keep)
```

```{r}
# Extract PCA scores and combine with the adjusted 'bclass', which is 'binary_class'
pca_scores <- pca_result$x

data_for_model <- data.frame(pca_scores, binary_class = claims_raw_filtered$bclass)
data_for_model$binary_class <- as.factor(data_for_model$binary_class)
```



```{r}
# Fix Warning: glm.fit: algorithm did not converge
binary_class_index <- which(names(data_for_model) == "binary_class")

# Choose the first 500 components (through Cumulative Variance Explained) and binary class column
data_for_model_reduced <- data_for_model[, c(1:500, binary_class_index)] 


names(data_for_model_reduced)
```


```{r}
# Fit logistic regression model using PCA components
logistic_model <- glm(binary_class ~ ., data = data_for_model_reduced, family = binomial)
```
### 5. Are binary class predictions improved using logistic principal component regression?
```{r}
library(pROC)

predicted_probs <- predict(logistic_model, type = "response")

# AUC after PCA
roc_curve <- roc(data_for_model_reduced$binary_class, predicted_probs)
auc(roc_curve)

# ROC Curve
plot(roc_curve)

```
Yes, the binary class predictions are significantly improved using logistic principal component regression compared to random guessing has an AUC of 0.5. After including 500 principal components, the AUC increased to 0.8268, which demonstrates a strong ability to distinguish between the two classes. 




## Preliminary Task 2 ()

### 1.Secondary Tokenization to obtain bigrams
```{r}
names(claims_raw) 
```
```{r}
# Tokenize the text to obtain bigrams
bigram_data <- claims_raw %>%
  unnest_tokens(bigram, content_combined, token = "ngrams", n = 2)


```

### 2.Fit Logistic Regression on word-tokenized data (unigrams) as baseline
```{r}
# What we do in task1  

# Fit logistic regression on PCA of word-tokenized data
pca_scores_words <- pca_result$x
word_model <- glm(binary_class ~ ., data = data_for_model_reduced, family = binomial)

# Predicted log-odds-ratios
word_predicted_log_odds <- predict(word_model, type = "link")


```
### 3.Input Predicted Log-Odds and PCA Scores of Bigrams
```{r}
# Fix error: memory limit 
# Count bigram frequencies across all documents
bigram_counts <- bigram_data %>%
  count(bigram) %>%
  filter(n > 5)  # Keep bigrams that occur more than 5 times

# Filter the bigram_data to include only frequent bigrams
bigram_data_filtered <- bigram_data %>%
  filter(bigram %in% bigram_counts$bigram)

# Create a smaller DTM
bigram_dtm <- bigram_data_filtered %>%
  count(.id, bigram) %>%
  cast_dtm(document = .id, term = bigram, value = n)

bigram_matrix <- as.matrix(bigram_dtm)

```

```{r}
bigram_pca_result <- prcomp(bigram_matrix, center = TRUE, scale. = TRUE)

```

```{r}
# Extract the top 10 principal components from bigram PCA
bigram_pca_scores <- bigram_pca_result$x
combined_data <- data.frame(
  log_odds = word_predicted_log_odds,
  bigram_pca_scores = bigram_pca_scores[, 1:10],  # Use top 10 bigram components
  binary_class = claims_raw_filtered$bclass
)
```


## Preliminary Task 2 Ver.2 ()
### 1.Bigram Tokenization
```{r}
# Bigram Tokenization
library(tidytext)

bigram_tokens <- claims_raw %>%
  unnest_tokens(output = bigram, input = content_combined, token = "ngrams", n = 2)

```
### 2.Filter Low-frequency Bigram
```{r}
# Filter frequent bigrams
bigram_counts <- bigram_tokens %>%
  count(bigram) %>%
  filter(n > 5)  # Only keep bigrams appearing more than 5 times

filtered_bigram_tokens <- bigram_tokens %>%
  filter(bigram %in% bigram_counts$bigram)
```

### 3.Construct the DTM of Bigram
```{r}
# Create a Document-Term Matrix for bigrams
bigram_dtm <- filtered_bigram_tokens %>%
  count(.id, bigram) %>%
  cast_dtm(document = .id, term = bigram, value = n)

```
### 4.PCA on Bigram's DTM
```{r}
# Convert to a dense matrix and perform PCA
bigram_matrix <- as.matrix(bigram_dtm)

# Perform PCA on the filtered bigram DTM
bigram_pca <- prcomp(bigram_matrix, center = TRUE, scale. = TRUE)

# Extract top N components (adjustable)
bigram_pca_scores <- bigram_pca$x[, 1:10]  # Use top 10 components

```

### 5. Combine Log-Odds of Word Prediction(unigram) and PCA Scores of Bigram

```{r}
# What we do in task1  

# Fit logistic regression on PCA of word-tokenized data
pca_scores_words <- pca_result$x
#word_model <- glm(binary_class ~ ., data = data_for_model_reduced, family = binomial)

# Predicted log-odds-ratios
predicted_log_odds <- predict(logistic_model, type = "link")

```

```{r}
# Step 1: Identify rows to keep
rows_to_keep <- intersect(
  names(predicted_log_odds),  # Use names() for vectors
  rownames(bigram_pca_scores)
)

# Step 2: Filter claims_raw_filtered to keep only matching rows
claims_raw_filtered <- claims_raw_filtered %>%
  filter(.id %in% rows_to_keep)

# Step 3: Subset predicted_log_odds and bigram_pca_scores to the same rows
predicted_log_odds <- predicted_log_odds[rows_to_keep]  # Simple indexing for vectors
bigram_pca_scores <- bigram_pca_scores[rows_to_keep, , drop = FALSE]  # Subset matrix

# Step 4: Combine aligned datasets
combined_features <- data.frame(
  log_odds_unigram = predicted_log_odds,
  bigram_pca_scores,
  binary_class = claims_raw_filtered$bclass  # Ensure alignment
)

```


```{r}
# Combine unigram log-odds and bigram PCA scores
combined_features <- data.frame(
  log_odds_unigram = predicted_log_odds,  # From your unigram model
  bigram_pca_scores,  # Top 10 bigram PCA scores
  binary_class = claims_raw_filtered$bclass  # Ensure consistent alignment
)

```



## Primary Task 1 ()

### 1.Load and Preprocess the Data
```{r}
load("data2/claims-raw.RData")
load("data2/claims-test.RData")
#load("data2/claims-clean-example.RData")
```

```{r}
#head(claims_raw)
#head(claims_clean_example)
```

```{r}

# Why use tryCatch: prevent the entire script from failing due to an issue with a single piece of HTML content
extract_content <- function(html_content) {
  tryCatch({
    # Read the HTML content
    page <- read_html(html_content)
    
    # Extract headers and paragraphs
    headers <- page %>% html_nodes("h1, h2, h3") %>% html_text()
    paragraphs <- page %>% html_nodes("p") %>% html_text()
    
    # Combine headers and paragraphs
    combined_content <- c(headers, paragraphs)
    return(paste(combined_content, collapse = " "))
  }, error = function(e) {
    # Return NA or an empty string if an error occurs
    return(NA)
  })
}

# Apply the function to the 'text_tmp' column
claims_raw$content_combined <- sapply(claims_raw$text_tmp, extract_content)
claims-test$content_combined <- sapply(claims-test$text_tmp, extract_content)

```

```{r}
library(tidytext)

# Tokenize and clean training data
train_tokens <- claims_raw %>%
  unnest_tokens(word, content_combined) %>%
  anti_join(stop_words, by = "word") %>%
  count(.id, word)

# Tokenize and clean test data
test_tokens <- claims_test %>%
  unnest_tokens(word, content_combined) %>%
  anti_join(stop_words, by = "word") %>%
  count(.id, word)
```


### 2. Vectorize Text Features
```{r}

```


### 3.Train Predictive Models
### 4.Generate Predictions
### 5.Export Deployable Model