---
title: "Summary of exploratory tasks"
author: 'Tess, Jiajia, Will, and Ivy'
date: today
---

### HTML scraping

To test out whether or not the headers improve predictions of whether or not our text is fraud or not, we modified the pre-processing functions to include the paragraph text as well as the headers. After having two different text processing functions, we fit binary classification models on both of the scraped data frames, with and without headers. After finding our predictions and computing the accuracy of our two models, we find that it is relatively similar, but in the end, including the headers does not improve our predictions. For sensitivity, our true positive rate, there was a value of 0.782 for the headers and paragraphs, while just the paragraph text had 0.822. For specificity, the true negative rate, there was an improvement from 0.746 to 0.767 when we didn't include the headers. Accuracy, also went up without headers, going from 0.765 to 0.797. Finally, our AUC value stayed the same at 0.841. So while there is not a big difference in our predictions if we include the headers or not, there is still a slight improvement if we stick with just the paragraph text and exclude the header content. 

### Bigrams


Through our analysis, we fit two different logistic regression models. For one of them, we word tokenized the data, and for the second one we performed a bigram tokenization of the data. For the second model, we used the log odds ratio from the first model and the principal components of the bigram data to fit our second logistic regression model. After comparing the two models, we found that the bigrams are able to capture more information that we can use to classify these texts as fraud or not. The bigram model had an AIC value of 2892.6 while the single word tokenization had a value of 3060.1, which indicates that the bigram model is a better fit for the data. 

